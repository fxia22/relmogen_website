

<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for Mobile Manipulation</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

     <!--FACEBOOK-->
     <meta property="og:image" content="http://svl.stanford.edu/projects/relmogen/img/pull_fig.jpg">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="http://http://svl.stanford.edu/projects/relmogen/"/>
    <meta property="og:title" content="ReLMoGen" />
    <meta property="og:description" content="Project page for ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for Mobile Manipulation" />

        <!--TWITTER-->
     <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ReLMoGen" />
    <meta name="twitter:description" content="Project page for ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for Mobile Manipulation" />
    <meta name="twitter:image" content="http://svl.stanford.edu/projects/relmogen/img/pull_fig.jpg" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <link rel="icon" type="image/png" href="img/seal_icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <!-- <script src="js/app.js"></script> -->
    <script>
        $(function () {
              $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
			
            <h2 class="col-md-12 text-center">
				
               ReLMoGen: Leveraging Motion Generation in <br >Reinforcement Learning for Mobile Manipulation </br>
<!--                 <small>
                    arXiv 2020
                </small> -->
            </h2>
			
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="http://fxia.me">
                          Fei Xia*
                        </a>
                        </br>Stanford University
                    </li>
                    <li>
                        <a href="https://www.chengshuli.me/">
                          Chengshu Li*
                        </a>
                        </br>Stanford University
                    </li>
                    <li>
                        <a href="https://robertomartinmartin.com/">
                            Roberto Martín-Martín
                        </a>
                        </br>Stanford University
                    </li>
                    <li>
                        <a href="https://orlitany.github.io/">
                          Or Litany
                        </a>
                        </br>Stanford University
                    </li>
                    <br>
                    
                    <li>
                        <a href="https://sites.google.com/view/alextoshev">
                          Alexander Toshev
                        </a>
                        </br>Robotics at Google
                    </li>
                    <li>
                        <a href="https://cvgl.stanford.edu/silvio/">
                          Silvio Savarese
                        </a>
                        </br>Stanford University
                    </li>
                </ul>
                *denotes equal contribution
            </div>
        </div>


        <div class="row">
                <div class="col-md-2 col-md-offset-5 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2008.07792">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <!--<li>
                            <a href="https://github.com/tancik/fourier-feature-networks">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
<!--                 <image src="img/teaser.png" class="img-responsive" alt="overview"><br>
 -->                <p class="text-justify">
Many Reinforcement Learning (RL) approaches use joint control signals (positions, velocities, torques) as action space for continuous control tasks. We propose to lift the action space to a higher level in the form of subgoals for a motion generator (a combination of motion planner and trajectory executor). We argue that, by lifting the action space and by leveraging sampling-based motion planners, we can efficiently use RL to solve complex, long-horizon tasks that could not be solved with existing RL methods in the original action space. We propose ReLMoGen -- a framework that combines a learned policy to predict subgoals and a motion generator to plan and execute the motion needed to reach these subgoals. To validate our method, we apply ReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation problems where interactions with the environment are required to reach the destination, and 2) Mobile Manipulation tasks, manipulation tasks that require moving the robot base. These problems are challenging because they are usually long-horizon, hard to explore during training, and comprise alternating phases of navigation and interaction. Our method is benchmarked on a diverse set of seven robotics tasks in photo-realistic simulation environments. In all settings, ReLMoGen outperforms state-of-the-art Reinforcement Learning and Hierarchical Reinforcement Learning baselines. ReLMoGen also shows outstanding transferability between different motion generators at test time, indicating a great potential to transfer to real robots.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <p class="text-justify">
                    Short on time? This video should capture the gist of our work. Enjoy!
	            </p>
                <div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;">
                   <video id="v0" width="100%" loop="" controls="">
                       <source src="img/relmogen_youtube_compressed.mp4" type="video/mp4">
                   </video>
               </div>
            </div>
        </div>
            

     <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
        </div>
		<div class="col-md-8 col-md-offset-2">
			
            <figure>
                <image src="img/method.png" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
			
	    </div>
		
		<div class="col-md-8 col-md-offset-2" style="margin-top: 15px; margin-bottom: 15px">
            We propose to integrate <b>Motion Generation</b> into a <b>Reinforcement Learning</b> loop to lift the action space from low-level robot commands <b><i>a</i></b> to subgoals for the motion generator <b><i>a′</i></b>; Our ReLMoGen solution maps observations and (possibly) task information to base or arm subgoals that the motion generator transforms into low-level robot commands. The mobile manipulation tasks that we are interested in can usually be decomposed into a sequence of base and arm subgoals (e.g. pushing open a door for Interactive Navigation).
	    </div>	

		<div class="col-md-8 col-md-offset-2">
			
            <figure>
                <image src="img/network.png" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
			
	    </div>
		
		<div class="col-md-8 col-md-offset-2" style="margin-top: 15px">
            We instantiate our ReLMoGen solution with two types of action parameterization and network architecture: SGP-D and SGP-R. SGP-D is based on <a data-delay='{"show":"100", "hide":"1000"}' data-toggle="tooltip" data-html="true" title="<a href='https://arxiv.org/abs/1312.5602'>Mnih, Volodymyr, et al. &#34;Playing atari with deep reinforcement learning&#34;. arXiv preprint arXiv:1312.5602 (2013).</a>">DQN</a> using discrete action space. We adopt a fully convolutional network structure that predicts Q-values for the base and arm subgoals spatially aligned with the local top-down map and first-person RGB-D view respectively. SGP-R is based on <a data-delay='{"show":"100", "hide":"1000"}' data-toggle="tooltip" data-html="true" title="<a href='https://arxiv.org/abs/1812.05905'>Haarnoja, Tuomas, et al. &#34;Soft actor-critic algorithms and applications.&#34; arXiv preprint arXiv:1812.05905 (2018).</a>">SAC</a> using continuous action space. The actor network directly predicts the base and arm subgoals and a binary variable that indicates whether to use base or arm for this subgoal.
        </div>
		
		
    </div>

     <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Tasks
            </h3>
        </div>
		<div class="col-md-8 col-md-offset-2">
        <figure>
            <image src="img/tasks.png" class="img-responsive" alt="overview">
            <figcaption>
            </figcaption>
        </figure>
		</div>
        <div class="col-md-8 col-md-offset-2" style="margin-top: 15px; margin-bottom: 15px">
            We stress test our method on a wide variety of seven robotics tasks including navigation, stationary arm control, interactive navigation and mobile manipulation. See below for a brief summary of each of our tasks.
        </div>
		<div class="col-md-8 col-md-offset-2">
            <b>PointNav:</b> the goal is to navigate from a random starting location and a random goal location without collision.
	    </div>
		<div class="col-md-8 col-md-offset-2">
            <b>TabletopReachM:</b> the goal is to reach a random goal location (represented as a red circle) on the table.
	    </div>
	    <div class="col-md-8 col-md-offset-2">
            <b>PushDoorNav:</b> the goal is to push a door open, and navigate to the goal location inside the room.
	    </div>
   	    <div class="col-md-8 col-md-offset-2">
            <b>ButtonDoorNav:</b> the goal is to press a button to open a door, and navigate to the goal location inside the room.
	    </div>
	    <div class="col-md-8 col-md-offset-2">
            <b>InteractiveObstaclesNav:</b> the goal is to push away a movable obstacle (represented as a red cuboid), and navigate to goal location behind the obstacles.
	    </div>
   	    <div class="col-md-8 col-md-offset-2">
            <b>ArrangeKitchenMM:</b> the goal is close as many cabinets and drawers as possible in the kitchen.
	    </div>
        <div class="col-md-8 col-md-offset-2">
            <b>ArrangeChairMM:</b> the goal is to tuck the chairs under the table.
	    </div>
    </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
               <h3>
                    Qualitative Results
               </h3>
         </div>
         <div class="col-md-8 col-md-offset-2" style="margin-top: 15px; margin-bottom: 15px">
             We showcase the qualitative results of our best performing policy in all the tasks.
         </div>
               <div class="col-md-4 col-md-offset-2">
                   <h4>
                       PointNav
                   </h4>
                   <video id="v1" width="100%" loop="" controls="">
                       <source src="img/random_nav.mp4" type="video/mp4">
                   </video>
               </div>
               <div class="col-md-4">
                   <h4>
                       TabletopReachM
                   </h4>
                   <video id="v2" width="100%" loop="" controls="">
                       <source src="img/tabletop_reaching.mp4" type="video/mp4">
                   </video>
               </div>
               <div class="col-md-4 col-md-offset-2">
                   <h4>
                       PushDoorNav
                   </h4>
                   <video id="v3" width="100%" loop="" controls="">
                       <source src="img/push_door.mp4" type="video/mp4">
                   </video>
               </div>
               <div class="col-md-4">
                   <h4>
                       ButtonDoorNav
                   </h4>
                   <video id="v4" width="100%" loop="" controls="">
                       <source src="img/button_door.mp4" type="video/mp4">
                   </video>
               </div>
               <div class="col-md-4 col-md-offset-2">
                   <h4>
                       InteractiveObstaclesNav
                   </h4>
                   <video id="v5" width="100%" loop="" controls="">
                       <source src="img/semantic_obstacles.mp4" type="video/mp4">
                   </video>
               </div>
               <div class="col-md-4">
                   <h4>
                       ArrangeKitchenMM
                   </h4>
                   <video id="v6" width="100%" loop="" controls="">
                       <source src="img/push_drawers.mp4" type="video/mp4">
                   </video>
               </div>

               <div class="col-md-4 col-md-offset-4">
                   <h4>
                       ArrangeChairMM
                   </h4>
                   <video id="v7" width="100%" loop="" controls="">
                       <source src="img/push_chairs.mp4" type="video/mp4">
                   </video>
               </div>

    </div>
     

     <div class="row">
        <div class="col-md-8 col-md-offset-2">
                <h3>
                    Quantitative Results
                </h3>
        </div>
		
		<div class="col-md-8 col-md-offset-2">
        <figure>
            <image src="img/reward.png" class="img-responsive" alt="overview">
            <figcaption>
            </figcaption>
        </figure>
		</div>
		
		<div class="col-md-8 col-md-offset-2" style="margin-top: 15px">
            Here we show reward curves for ReLMoGen and the baselines (<a data-delay='{"show":"100", "hide":"1000"}' data-toggle="tooltip" data-html="true" title="<a href='https://arxiv.org/abs/1812.05905'>Haarnoja, Tuomas, et al. &#34;Soft actor-critic algorithms and applications.&#34; arXiv preprint arXiv:1812.05905 (2018).</a>">SAC</a> and <a data-delay='{"show":"100", "hide":"1000"}' data-toggle="tooltip" data-html="true" title="<a href='https://arxiv.org/abs/1910.11432'>Li, Chengshu, et al. &#34;HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators.&#34; Conference on Robot Learning. 2020.</a>">HRL4IN</a>). ReLMoGen achieves higher reward with the same number of environment episodes and higher overall task completion for all seven tasks while the baselines often converge prematurely to sub-optimal solutions. 
	    </div>
		
    </div>


     <div class="row">
        <div class="col-md-8 col-md-offset-2">
                <h3>
                    Analysis
                </h3>
        </div>
		
		<div class="col-md-6 col-md-offset-3">
        <figure>
            <image src="img/exploration.png" class="img-responsive" alt="overview">
            <figcaption>
            </figcaption>
        </figure>
		</div>
		
		<div class="col-md-8 col-md-offset-2" style="margin-top: 15px; margin-bottom: 15px">
			Here we show ReLMoGen is better at exploration than the SAC baseline. (a) shows the 2D projection of latent state space: SAC traverses nearby states with low-level actions, while ReLMoGen-R jumps between distant states linked by a motion plan. (b) shows the physical locations visited by ReLMoGen-R and SAC in 100 episodes: ReLMoGen-R covers a much larger area. (c) shows a top-down map of meaningful interactions (duration ≥1s) during exploration. ReLMoGen-R is able to interact with the environment more than SAC.
	    </div>
		
		<div class="col-md-8 col-md-offset-2">
        <figure>
            <image src="img/q-value-vis.png" class="img-responsive" alt="overview">
            <figcaption>
            </figcaption>
        </figure>
		</div>
		
		<div class="col-md-8 col-md-offset-2" style="margin-top: 15px; margin-bottom: 15px">
			Here we show the visualization of ReLMoGen-D action maps during evaluation. The image pairs contain the input RGB frames on the left and normalized predicted Q-value maps on the right. The predicted Q-value spikes up at image locations that enable useful interactions, e.g. buttons, cabinet door leaves, and chairs.
	    </div>
		<div class="col-md-3 col-md-offset-2">
        <figure>
            <image src="img/new_robot2.jpg" class="img-responsive" alt="overview">
            <figcaption>
            </figcaption>
        </figure>
		</div>
        <div class="col-md-5">
            <video id="v8" width="100%" loop="" controls="">
                <source src="img/movo.mp4" type="video/mp4">
            </video>
        </div>

		<div class="col-md-8 col-md-offset-2" style="margin-top: 15px">
			Here we show the qualitative results of our policy transfer to a new robot Movo. After fine-tuning, our policy quickly adapts to the new embodiment and learns to set feasible subgoals accordingly.
	    </div>
	
	</div>
	
	



      <!--   <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    Random Fourier features were first proposed in the seminal work of <a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">Rahimi & Recht (2007)</a>.
                </p>
                <p class="text-justify">
                    The neural tangent kernel was introduced in <a href="https://arxiv.org/abs/1806.07572">Jacot et al. (2018)</a>. 
                </p>
                <p class="text-justify">
                    We relied on the excellent open source projects <a href="https://github.com/google/jax">JAX</a> and <a href="https://github.com/google/neural-tangents">Neural Tangents</a> for training networks and calculating neural tangent kernels.
                </p>
                <p class="text-justify">
                    In own previous work on <em>neural radiance fields</em> (<a href="https://www.matthewtancik.com/nerf">NeRF</a>), we were surprised to find that a "positional encoding" of input coordinates helped networks learn significantly higher frequency details, inspiring our exploration in this project.
                </p>
                <p class="text-justify">
                    <a href="https://vsitzmann.github.io/siren/">Sitzmann et al. (2020)</a> concurrently introduced <em>sinusoidal representation networks</em> (SIREN), demonstrating exciting progress in coordinate based MLP representations by using a sine function as the nonlinearity between <em>all</em> layers in the network. This allows the MLPs to accurately represent first and second order derivatives of low dimensional signals. 
                </p>
                <p class="text-justify">
                    You can find code to replicate all our experiments on <a href="https://github.com/tancik/fourier-feature-networks">GitHub</a>, but if you just want to try experimenting with the images used on this webpage you can find the uncompressed originals here: 
                    <a href="img/lion_orig.png">Lion</a>,
                    <a href="img/greece_orig.png">Greece</a>,
                    <a href="img/fox_orig.png">Fox</a>.
                </p>
            </div>
        </div> -->
        

<!--         <div class="row" id="header_img">
            <figure class="col-md-8 col-md-offset-2">
                <image src="img/llff_teaser.png" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
                
        </div> -->
            
         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly rows="8" >
@article{xiali2020relmogen, 
    title={ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for Mobile Manipulation},
    author={Xia, Fei and Li, Chengshu and Mart{\'\i}n-Mart{\'\i}n, Roberto and Litany, Or and Toshev, Alexander and Savarese, Silvio }, 
    journal={arXiv preprint arXiv:2008.07792}, 
    year={2020} 
}</textarea>
                </div>
            </div>
        </div> 

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Full Paper
                </h3>
                <p>
                    With Appendix detailing task definition, evaluation metrics, algorithm description,
                    network structure, training procedure, and hyperparameters.
                </p>
                <embed src="img/relmogen.pdf#toolbar=0&navpanes=0&scrollbar=0" type="application/pdf" width="100%"
                    height="1000px" />
            </div>

        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank Google for providing cloud computing credits for this projects. This project is also supported by HAI AWS grant. The authors would like to thank members from Mobility team of Robotics at Google and PAIR team from Stanford Vision and Learning Group for valuable feedbacks on early versions of this project.
                    <br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>


    </div>
    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=100&t=tt&d=eWSnDJ_QEhvNBiiW_bFlg-z7p8RMHz8Y-5j5Xmi7Xqs&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
</body>
</html>
